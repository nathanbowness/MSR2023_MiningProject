{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22e62c67-babe-4f18-b04c-364826f10a93",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import markdown\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "import csv\n",
    "import nltk\n",
    "import nltk.data\n",
    "nltk.download(\"popular\")\n",
    "from nltk import pos_tag\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "from nltk import pos_tag_sents\n",
    "from nltk.corpus import stopwords\n",
    "from nltk import pos_tag\n",
    "from nltk.corpus import wordnet as wn\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.sentiment.util import mark_negation\n",
    "import emoji\n",
    "import contractions\n",
    "import json\n",
    "\n",
    "def get_lemmatizer_pos(pos):\n",
    "    \"\"\"Retrieves the POS-tag in a WordNet-compatible form. It returns a noun tag by default if the word is not\n",
    "    an adjective, verb, or adverb.\n",
    "\n",
    "    Args:\n",
    "        pos: NTLK POS-tag.\n",
    "\n",
    "    Returns:\n",
    "        A WordNet-compatible POS-tag string.\n",
    "    \"\"\"\n",
    "    pos_start = pos[0]  # Takes the first letter to simplify the POS tag\n",
    "    if pos_start == \"J\":\n",
    "        return wn.ADJ\n",
    "    elif pos_start == \"V\":\n",
    "        return wn.VERB\n",
    "    elif pos_start == \"R\":\n",
    "        return wn.ADV\n",
    "    else:\n",
    "        return wn.NOUN\n",
    "\n",
    "def tokenize_normalize(\n",
    "    context,\n",
    "    sentence_tokenizer=nltk.data.load(\"tokenizers/punkt/english.pickle\"),\n",
    "    lemmatizer=WordNetLemmatizer(),\n",
    "    tokenizer=TweetTokenizer(preserve_case=False), # This tokenizer can handle URLs better\n",
    "    tokenize_numbers=False,\n",
    "    tokenize_urls=False,\n",
    "    deemojize=True,\n",
    "    remove_punct=True,\n",
    "    handle_negation=True\n",
    "):\n",
    "    stop_words = stopwords.words(\"english\")\n",
    "\n",
    "    punctuations = \"!\\\"“”#$%&'‘’()*+,-./:;<=>?@[\\]^_`{|}~‍\"\n",
    "\n",
    "    sentences = sentence_tokenizer.tokenize(context)\n",
    "\n",
    "    sentences_tokens = [tokenizer.tokenize(sentence) for sentence in sentences]\n",
    "\n",
    "    tokens_pos = [\n",
    "        pos_tag\n",
    "        for pos_tags in pos_tag_sents(sentences_tokens)\n",
    "        for pos_tag in pos_tags\n",
    "    ]\n",
    "\n",
    "    lemmas = [\n",
    "        lemmatizer.lemmatize(token[0], pos=get_lemmatizer_pos(token[1]))\n",
    "        for token in tokens_pos\n",
    "    ]\n",
    "    \n",
    "    if handle_negation:\n",
    "        lemmas = mark_negation(lemmas)\n",
    "\n",
    "    filtered_lemmas = []\n",
    "    last_filtered_lemma_index = None\n",
    "    last_filtered_lemma = None\n",
    "    for lemma_index, lemma in enumerate(lemmas):\n",
    "\n",
    "        # Removing variation selectors such as hair/skin color and gender for\n",
    "        # emojis since they cause noise and tokenization problems:\n",
    "        if re.sub(\"[\\\\uFE00-\\\\uFE0F♂♀‍]+\", \"\", lemma) == \"\":\n",
    "            continue\n",
    "\n",
    "        # Filters stop words (considers negations):\n",
    "        if lemma.replace(\"_NEG\", \"\") in stop_words:\n",
    "            continue\n",
    "\n",
    "        # Filters the lemma by searching for \"https://,\" \"http://,\" or \"www.\" using regular expression. If one\n",
    "        # of them exists, they are not retrieved. Regular expression may seem daunting at first. It is not\n",
    "        # mandatory, but you can check tutorials like this: https://regexone.com/lesson/introduction_abcs\n",
    "        if re.search(\"(https?:\\/\\/)|(www\\.)\", lemma):\n",
    "            if tokenize_urls:\n",
    "                lemma = \"\"\n",
    "            else:\n",
    "                continue\n",
    "\n",
    "        # Deemojizes emojis using emeji package (considers negations):\n",
    "        if deemojize:\n",
    "            lemma = emoji.demojize(lemma)\n",
    "\n",
    "        # Filters punctuation (considers negations):\n",
    "        if (\n",
    "            remove_punct\n",
    "            and lemma.replace(\"_NEG\", \"\").translate(\n",
    "                lemma.maketrans(\"\", \"\", punctuations)\n",
    "            )\n",
    "            == \"\"\n",
    "        ):\n",
    "            continue\n",
    "\n",
    "        # Tries to convert a number from string to float while also handling commas and percentage signs. If\n",
    "        # the token is a number, it is transformed to \"\" token or not retrieved. If not, it silently\n",
    "        # ignores the exception and continues.\n",
    "        try:\n",
    "            float(lemma.replace(\",\", \"\").replace(\"%\", \"\"))\n",
    "            if tokenize_numbers:\n",
    "                lemma = \"\"\n",
    "            else:\n",
    "                continue\n",
    "        except:\n",
    "            pass\n",
    "\n",
    "        filtered_lemmas.append(lemma)\n",
    "    \n",
    "    return filtered_lemmas"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2a19287-7294-43f1-bb47-abfdf04dae88",
   "metadata": {},
   "source": [
    "Processes the paragraphs that come before the first H2 title in the readme file for each repository:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "f3e99d6e-de43-4cc9-9a21-69198e9e99cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing Java repositories...\n",
      "Processing Python repositories...\n"
     ]
    }
   ],
   "source": [
    "languages = [\"Java\", \"Python\"]\n",
    "readme_folder = \"readmes\"\n",
    "\n",
    "for language in languages:\n",
    "    \n",
    "    print(f\"Processing {language} repositories...\")\n",
    "    \n",
    "    with open(f\"processed_readmes_{language.lower()}.csv\", \"w\", newline=\"\", encoding=\"UTF-8\") as csv_file:\n",
    "        csv_writer = csv.writer(csv_file, delimiter=\";\", quotechar=\"'\", quoting=csv.QUOTE_MINIMAL)\n",
    "        csv_writer.writerow([\"ID\", \"OWNER\", \"NAME\", \"CONTEXTS\"])\n",
    "    \n",
    "        candidate_repos = pd.read_csv(f\"candidate_repos_{language.lower()}.csv\", delimiter=\";\")\n",
    "\n",
    "        for row_id, row in candidate_repos.iterrows():\n",
    "            readme_path = f\"{readme_folder}/{row.OWNER}_{row.NAME}_readme.md\"\n",
    "            if not os.path.isfile(readme_path):\n",
    "                continue\n",
    "\n",
    "            with open(readme_path, encoding=\"UTF-8\") as f:\n",
    "                readme = f.read()\n",
    "                \n",
    "            # Not all readme files are perfectly parseable. Some extra steps were needed both before and after parsing the file.\n",
    "            if \"```\" in readme:\n",
    "                readme = readme.split(\"```\")\n",
    "                if readme[0] == \"\":\n",
    "                    readme = \"\\n\".join(readme[1::2])\n",
    "                else:\n",
    "                    readme = \"\\n\".join(readme[0::2])\n",
    "\n",
    "            readme = BeautifulSoup(markdown.markdown(readme), \"html.parser\")\n",
    "            for img in readme.select(\"img\"):\n",
    "                img.extract()\n",
    "\n",
    "            extracted_text = []\n",
    "            for e in readme:\n",
    "                if e.name == \"h2\":\n",
    "                    break\n",
    "\n",
    "                # Clear-text paragraphs are only retrieved if they satisfy certain conditions\n",
    "                if e.name == \"p\":\n",
    "                    e = re.sub(\"<.*?>|&([a-z0-9]+|#[0-9]{1,6}|#x[0-9a-f]{1,6});\", \"\", e.text)\n",
    "                    e = contractions.fix(e)\n",
    "                    if len(cleaned.strip()) > 3:\n",
    "                        extracted = \" \".join([e for e in e.split(\"\\n\") if not e.startswith(\".. image::\") if not e.startswith(\"image:\") if not e.startswith(\"  :\")])\n",
    "                        if len(set(extracted.strip())) > 3:\n",
    "                            extracted = tokenize_normalize(extracted)\n",
    "                            extracted_text.append(extracted)\n",
    "            \n",
    "            if extracted_text:\n",
    "                csv_writer.writerow([row.ID, row.OWNER, row.NAME, json.dumps(extracted_text, ensure_ascii=False)])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
